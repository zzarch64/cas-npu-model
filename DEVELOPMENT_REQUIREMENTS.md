# CAS NPU Extension å¼€å‘éœ€æ±‚æ‰‹å†Œ

## é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ—¨åœ¨ä¸º PyTorch æä¾› CAS NPU åç«¯æ”¯æŒï¼Œå®ç°æ ¸å¿ƒç®—å­ä»¥æ”¯æŒæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯ LLMï¼‰åœ¨ NPU ä¸Šçš„è¿è¡Œã€‚

## å¼€å‘è¿›åº¦

### âœ… å·²å®ŒæˆåŠŸèƒ½

#### 1. çŸ©é˜µä¹˜æ³•ç®—å­ (MM/BMM)

**çŠ¶æ€**: âœ… å·²å®Œæˆå¹¶æµ‹è¯•é€šè¿‡

**å®ç°å†…å®¹**:

##### Runtime å±‚ (`cas_npu_runtime.h/cpp`)
- âœ… `casNpuMatMul`: çŸ©é˜µä¹˜æ³• `output = input1 @ input2`
  - è¾“å…¥: `input1: [M, K]`, `input2: [K, N]`
  - è¾“å‡º: `output: [M, N]`
- âœ… `casNpuBatchMatMul`: æ‰¹é‡çŸ©é˜µä¹˜æ³• `output = input1 @ input2`
  - è¾“å…¥: `input1: [B, M, K]`, `input2: [B, K, N]`
  - è¾“å‡º: `output: [B, M, N]`

##### PyTorch ç®—å­æ³¨å†Œ (`cas_npu_ops.cpp`)
- âœ… `cas_npu_mm`: PyTorch çŸ©é˜µä¹˜æ³•ç®—å­
- âœ… `cas_npu_bmm`: PyTorch æ‰¹é‡çŸ©é˜µä¹˜æ³•ç®—å­
- âœ… å·²æ³¨å†Œåˆ° PrivateUse1 åç«¯

**æµ‹è¯•ç»“æœ**:
- âœ… åŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼ˆç²¾åº¦è¯¯å·® < 1e-5ï¼‰
- âœ… Linear å±‚æµ‹è¯•é€šè¿‡
- âœ… Attention é£æ ¼ bmm æµ‹è¯•é€šè¿‡
- âœ… å¤šå±‚ Linear (Transformer FFN) æµ‹è¯•é€šè¿‡

**ç›¸å…³æ–‡ä»¶**:
- `runtime/cas_npu_runtime.h` - å‡½æ•°å£°æ˜
- `runtime/cmodel/simulator.cpp` - cmodel å®ç°
- `backend/cas_npu_ops.cpp` - PyTorch ç®—å­åŒ…è£…
- `test/test_qwen0.5B.py` - æµ‹è¯•è„šæœ¬

**æ€§èƒ½è¯´æ˜**:
- å½“å‰å®ç°ä½¿ç”¨ç®€å•çš„ä¸‰é‡å¾ªç¯ï¼Œé€‚åˆåŠŸèƒ½éªŒè¯å’Œæ­£ç¡®æ€§æµ‹è¯•
- ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ BLAS åº“æˆ–è®¾å¤‡ä¸“ç”¨çŸ©é˜µä¹˜æ³•åº“è¿›è¡Œä¼˜åŒ–

**åœ¨ LLM ä¸­çš„åº”ç”¨**:
- **mm**: Linear å±‚ã€FFNã€è¾“å‡ºæŠ•å½±
- **bmm**: Attention è®¡ç®— (`Q @ K^T`)ã€Attention è¾“å‡º (`scores @ V`)ã€å¤šå¤´æ³¨æ„åŠ›

---

## ğŸ“‹ å¾…å¼€å‘éœ€æ±‚

### é«˜ä¼˜å…ˆçº§

#### 1. Softmax ç®—å­
**éœ€æ±‚æè¿°**: å®ç° softmax å½’ä¸€åŒ–ç®—å­ï¼Œç”¨äº Attention æœºåˆ¶ä¸­çš„å½’ä¸€åŒ–

**æŠ€æœ¯è¦æ±‚**:
- æ”¯æŒä»»æ„ç»´åº¦çš„ softmax
- æ”¯æŒæŒ‡å®šç»´åº¦è¿›è¡Œå½’ä¸€åŒ–
- æ•°å€¼ç¨³å®šæ€§è€ƒè™‘ï¼ˆé˜²æ­¢æº¢å‡ºï¼‰

**åº”ç”¨åœºæ™¯**:
- Attention scores å½’ä¸€åŒ–: `softmax(Q @ K^T / sqrt(d_k))`

**çŠ¶æ€**: â³ å¾…å¼€å‘

---

#### 2. Layer Normalization ç®—å­
**éœ€æ±‚æè¿°**: å®ç°å±‚å½’ä¸€åŒ–ç®—å­ï¼ŒTransformer æ¶æ„çš„æ ¸å¿ƒç»„ä»¶

**æŠ€æœ¯è¦æ±‚**:
- æ”¯æŒå¯å­¦ä¹ çš„ç¼©æ”¾å’Œå¹³ç§»å‚æ•°
- æ”¯æŒæŒ‡å®šå½’ä¸€åŒ–ç»´åº¦
- æ”¯æŒ epsilon å‚æ•°é˜²æ­¢é™¤é›¶

**åº”ç”¨åœºæ™¯**:
- Transformer ä¸­çš„ Pre-LN å’Œ Post-LN
- æ¯ä¸ª Transformer å±‚çš„å‰åå½’ä¸€åŒ–

**çŠ¶æ€**: â³ å¾…å¼€å‘

---

#### 3. æ¿€æ´»å‡½æ•°
**éœ€æ±‚æè¿°**: å®ç°å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°

**å…·ä½“éœ€æ±‚**:
- **GELU**: Gaussian Error Linear Unitï¼ŒTransformer ä¸­å¸¸ç”¨
- **SiLU**: Swish æ¿€æ´»å‡½æ•°ï¼ŒLLM ä¸­å¸¸ç”¨

**åº”ç”¨åœºæ™¯**:
- FFN ä¸­çš„æ¿€æ´»å‡½æ•°
- æ›¿ä»£ ReLU ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½

**çŠ¶æ€**: â³ å¾…å¼€å‘

---

### ä¸­ä¼˜å…ˆçº§

#### 4. Embedding ç®—å­
**éœ€æ±‚æè¿°**: å®ç°è¯åµŒå…¥å±‚ï¼Œå°† token ID æ˜ å°„ä¸ºå‘é‡è¡¨ç¤º

**æŠ€æœ¯è¦æ±‚**:
- æ”¯æŒå¯å­¦ä¹ çš„åµŒå…¥çŸ©é˜µ
- æ”¯æŒ padding_idx
- é«˜æ•ˆçš„æŸ¥æ‰¾æ“ä½œ

**åº”ç”¨åœºæ™¯**:
- LLM çš„è¾“å…¥åµŒå…¥å±‚
- è¾“å‡ºæŠ•å½±å±‚ï¼ˆlogitsï¼‰

**çŠ¶æ€**: â³ å¾…å¼€å‘

---

#### 5. Scaled Dot-Product Attention
**éœ€æ±‚æè¿°**: å®ç°ä¼˜åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ•´åˆ Qã€Kã€V çš„è®¡ç®—

**æŠ€æœ¯è¦æ±‚**:
- æ•´åˆ Q @ K^Tã€scalingã€softmaxã€ä¸ V çš„ä¹˜æ³•
- æ”¯æŒ dropoutï¼ˆå¯é€‰ï¼‰
- æ”¯æŒ maskï¼ˆå¯é€‰ï¼‰
- æ€§èƒ½ä¼˜åŒ–ï¼ˆå‡å°‘ä¸­é—´å†…å­˜åˆ†é…ï¼‰

**åº”ç”¨åœºæ™¯**:
- Transformer çš„æ ¸å¿ƒæ³¨æ„åŠ›æœºåˆ¶
- æ›¿ä»£åˆ†åˆ«è°ƒç”¨ bmm å’Œ softmax çš„ç»„åˆ

**çŠ¶æ€**: â³ å¾…å¼€å‘

---

### ä½ä¼˜å…ˆçº§ / æ€§èƒ½ä¼˜åŒ–

#### 6. çŸ©é˜µä¹˜æ³•æ€§èƒ½ä¼˜åŒ–
**éœ€æ±‚æè¿°**: ä¼˜åŒ–å½“å‰çš„ä¸‰é‡å¾ªç¯å®ç°

**ä¼˜åŒ–æ–¹å‘**:
- é›†æˆ BLAS åº“ï¼ˆOpenBLAS, MKLï¼‰
- è°ƒç”¨è®¾å¤‡ä¸“ç”¨çŸ©é˜µä¹˜æ³•åº“
- ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
- æ”¯æŒåˆ†å—è®¡ç®—

**çŠ¶æ€**: â³ å¾…ä¼˜åŒ–

---

#### 7. å…¶ä»–å¸¸ç”¨ç®—å­
**éœ€æ±‚æè¿°**: æ ¹æ®å®é™…ä½¿ç”¨éœ€æ±‚è¡¥å……å…¶ä»–ç®—å­

**å¯èƒ½çš„ç®—å­**:
- `add`, `mul`, `div` - åŸºç¡€ç®—æœ¯è¿ç®—
- `transpose`, `reshape` - å¼ é‡æ“ä½œ
- `concat`, `split` - å¼ é‡æ‹¼æ¥å’Œåˆ†å‰²
- `dropout` - æ­£åˆ™åŒ–
- `relu`, `tanh` - å…¶ä»–æ¿€æ´»å‡½æ•°

**çŠ¶æ€**: â³ å¾…è¯„ä¼°

---

## å¼€å‘è®¡åˆ’

### Phase 1: æ ¸å¿ƒçŸ©é˜µè¿ç®— âœ…
- [x] MM (çŸ©é˜µä¹˜æ³•)
- [x] BMM (æ‰¹é‡çŸ©é˜µä¹˜æ³•)

### Phase 2: Attention æœºåˆ¶æ”¯æŒ
- [ ] Softmax
- [ ] Scaled Dot-Product Attention

### Phase 3: Transformer å®Œæ•´æ”¯æŒ
- [ ] Layer Normalization
- [ ] GELU/SiLU æ¿€æ´»å‡½æ•°
- [ ] Embedding

### Phase 4: æ€§èƒ½ä¼˜åŒ–
- [ ] çŸ©é˜µä¹˜æ³•æ€§èƒ½ä¼˜åŒ–
- [ ] å…¶ä»–ç®—å­ä¼˜åŒ–

---

## æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•
- æ¯ä¸ªç®—å­éƒ½éœ€è¦ç‹¬ç«‹çš„å•å…ƒæµ‹è¯•
- ä¸ CPU å®ç°å¯¹æ¯”éªŒè¯æ­£ç¡®æ€§
- æµ‹è¯•è¾¹ç•Œæƒ…å†µï¼ˆç©ºå¼ é‡ã€å•å…ƒç´ ç­‰ï¼‰

### é›†æˆæµ‹è¯•
- æµ‹è¯•ç®—å­ç»„åˆï¼ˆå¦‚ Linear + ReLU + Linearï¼‰
- æµ‹è¯•å®Œæ•´çš„æ¨¡å‹ç»„ä»¶ï¼ˆå¦‚ Transformer Layerï¼‰

### æ¨¡å‹æµ‹è¯•
- åœ¨çœŸå®æ¨¡å‹ä¸Šæµ‹è¯•ï¼ˆå¦‚ Qwen 0.5Bï¼‰
- éªŒè¯ç«¯åˆ°ç«¯çš„æ­£ç¡®æ€§

---

## å¤‡æ³¨

- å½“å‰å®ç°ä½¿ç”¨ cmodel è¿›è¡ŒåŠŸèƒ½éªŒè¯
- ç”Ÿäº§ç¯å¢ƒéœ€è¦å®ç° FPGA æˆ–å®é™…ç¡¬ä»¶ç‰ˆæœ¬
- æ‰€æœ‰ç®—å­éƒ½éœ€è¦åŒæ—¶æ”¯æŒ cmodel å’Œå®é™…ç¡¬ä»¶ä¸¤ç§å®ç°
